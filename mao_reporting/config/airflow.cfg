[core]
# The folder where your airflow DAGs are located
dags_folder = /path/to/your/project/dags

# The folder where airflow should store its log files
base_log_folder = /path/to/your/project/logs

# The executor class that airflow should use. For local development, SequentialExecutor is fine
# For production, you might want to use CeleryExecutor or KubernetesExecutor
executor = LocalExecutor

# The SqlAlchemy connection string to the metadata database
# Default uses sqlite, but for production consider PostgreSQL or MySQL
sql_alchemy_conn = sqlite:////path/to/your/project/airflow.db

# The encoding for the databases
sql_engine_encoding = utf-8

# Whether to load the DAG examples
load_examples = False

# The amount of parallelism as a setting to the executor
parallelism = 4

# The number of task instances allowed to run concurrently
max_active_tasks_per_dag = 8

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 4

# Whether to load the default connections
default_timezone = utc

# Secret key to save connection passwords in the db
fernet_key = 

# Default owner assigned to the created DAGs
default_owner = airflow

[webserver]
# The host and port of the webserver
web_server_host = 0.0.0.0
web_server_port = 8080

# The time the webserver waits before killing gunicorn master that doesn't respond
web_server_worker_timeout = 120

# Number of workers to refresh at a time when refreshing DAGs
worker_refresh_batch_size = 1

# Secret key for flask app
secret_key = temporary_key_for_local_dev

# The backend used to store user authentication
authenticate = True
auth_backend = airflow.api.auth.backend.basic_auth

# The default UI theme
dag_default_view = grid
dag_orientation = LR

# Number of workers to run the Gunicorn web server
workers = 4

# The worker class gunicorn should use
worker_class = sync

# Enable the experimental API
enable_proxy_fix = True

[scheduler]
# How often the scheduler should run (in seconds)
scheduler_heartbeat_sec = 5

# How often to scan the DAGs directory for new files
min_file_process_interval = 30

# The scheduler runs periodically check on the health of the DAGs
job_heartbeat_sec = 5

# How many seconds to wait between file-parsing loops
processor_poll_interval = 1

# How often should the scheduler check for orphaned tasks and SchedulerJobs
orphaned_tasks_check_interval = 300

# Enable the scheduler zombies resurrection
dag_dir_list_interval = 300

# How long before timing out a python file import
dagbag_import_timeout = 30

# How long before timing out a DagFileProcessor
parsing_processes = 2

[operators]
# Path to the folder containing custom operator plugins
plugins_folder = /path/to/your/project/plugins

[logging]
# Folder where airflow should write logs
logging_level = INFO
log_format = %(asctime)s - %(name)s - %(levelname)s - %(message)s
simple_log_format = %(asctime)s %(levelname)s - %(message)s

# Log processor expiration time
log_processors_timeout = 30

[metrics]
# Send metrics to statsd
statsd_on = False
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow

[smtp]
# If you want airflow to send emails on retry/failure, configure an SMTP server
smtp_host = smtp.example.com
smtp_starttls = True
smtp_ssl = False
smtp_port = 587
smtp_mail_from = airflow@example.com
smtp_user = your_username
smtp_password = your_password

[email]
# Email report configuration
email_backend = airflow.utils.email.send_email_smtp

[celery]
# For using the CeleryExecutor
broker_url = redis://redis:8080/0
result_backend = db+postgresql://postgres:airflow@postgres/airflow

celery_app_name = airflow.executors.celery_executor
worker_concurrency = 8

[api]
# Enable the API
auth_backend = airflow.api.auth.backend.basic_auth
access_control_allow_origins = *

[dask]
# For using the DaskExecutor
cluster_address = 127.0.0.1:8786

[elasticsearch]
# Elasticsearch host
host = localhost
log_id_template = {dag_id}-{task_id}-{execution_date}-{try_number}
end_of_log_mark = end_of_log

[kubernetes]
# For using KubernetesExecutor
# The namespace where airflow workers should be created
namespace = airflow
# The name of the Kubernetes ConfigMap containing the airflow configuration
airflow_configmap = airflow-configmap
# The name of the Kubernetes ConfigMap containing DAGs
dags_in_image = True